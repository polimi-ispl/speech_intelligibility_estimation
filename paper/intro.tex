\section{Introduction}\label{sec:intro}

Thanks to the recent advances in technology, audio communication systems are becoming more and more pervasive and diverse. In fact, nowadays a conversation can happen not only face-to-face but also on a variety of channels, like phone calls, VoIP or voice messages on instant messaging platforms.  

At the same time the ability of monitoring environmental, digital or phone communications has become an urgent necessity for national security issues. These technologies can effectively assist law enforcement agencies in foiling terrorist attacks or revealing harmful intents. The aforementioned heterogeneity of the communication channels has produced in this sense both advantages and drawbacks.  


On one side, information gathering has become easier and ubiquitous. Regarding environmental monitoring, audio surveillance devices are getting cheaper and easier to deploy. Subsequently, the number of control spots can be further increased, ensuring higher coverage, for example, in large public spaces. Concerning digital or phone wire taps, \textit{why is easier to collect data? because we have so many communication channels? }


On the other side, a blind and massive data collection can results in huge databases whose manual inspection might be infeasible. Tools like automatic transcription agents would be of great help for both investigative and surveillance purposes. However, given the diversity of context in which data is collected,the audio excerpts can be affected by several types of noise which can degrade the quality of the records and compromise the intelligibility of a possible relevant conversation. 


For these reasons it is needed to develop automatic and intelligent methods that allow to speed up the analysis of these huge corpora but at the same time take in account the variety of the involved devices and environments. Depending on the characteristics of the collected audio excerpts, these systems should be able to extract relevant information while guaranteeing control to the human user. 


In this work we try to meet these needs by proposing a framework able to evaluate the quality of noisy speech recordings. Given the specified context, this evaluation is based on the estimation of the likelihood of obtaining reliable transcript using a generic automatic speech-to-text engine. 


Our method can be employed in two different applicative contexts. The first one is for \textit{not sure what is the meaning of the bullet point. We want to propose an index to be exploited for tuning automatic transcription agents? or we want to compute the intelligibility index on large databases to evaluate the quality of a specific dataset?}.
The second one is, as already mentioned, to provide a useful tool in managing data acquired by audio surveillance systems. The model provides a quality feedback to investigators, that will be able to discriminate reliable from non reliable automatically extracted transcriptions.
 

The main rationale is to use a data-driven approach. Our system is composed of two main blocks. In the first one, a suitable set of features is extracted from the original audio signal. This features provide a numerical description of the fundamental characteristics of the signal analysed. Then, both a regressor and a classificator are designed and trained to predict the quality level (or a discrete label) of noisy speech audio signal. 


The implemented model has been tested on a large dataset of transcribed speech signal augmented with several type of noises. The ground truth has been obtained starting from the variation between the original transcription and the transcription of the noisy signal.

\begin{itemize}


	\item Audio communication systems are becoming more pervasive and diverse (e.g., phone calls, voip, voice messages, etc.).
	
	\item For national security reason (e.g., to avoid terroristic attacks, uncover malicious things, etc.), the ability of monitoring communications through phone tapping or environmental monitoring in several situations is an urgent necessity.
	
	\item Fortunately, audio surveillance devices are increasingly cheaper and easier to deploy.
	\item However, a blind and massive data collection might result in huge databases whose analysis might become unfeasible.
	\item It is therefore needed to develop methods that automatically enable to speed the analysis of these huge corpora of collected audio excerpts.
	\item In this paper we propose a framework to evaluate the quality of noisy speech recordings estimating the likelihood of obtaining a reliable transcript.
	\item Two applications: i) to process large databases with automatic speech-to-text transcriptors; ii) to provide a quality feedback to any investigator deploying an audio surveillance system.
	\item The main rationale is to use ...
	\item Results obtained on a lots of data show that ...
\end{itemize}


\cite{Piva2013}
